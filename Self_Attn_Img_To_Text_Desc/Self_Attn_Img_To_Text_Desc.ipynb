{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T08:07:14.743946100Z",
     "start_time": "2023-12-29T08:07:12.316010Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XUE HUAN\\AppData\\Local\\Temp\\ipykernel_8684\\834786935.py:1: DeprecationWarning: 'cgi' is deprecated and slated for removal in Python 3.13\n",
      "  from cgi import test\n"
     ]
    }
   ],
   "source": [
    "from cgi import test\n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T08:07:15.839577900Z",
     "start_time": "2023-12-29T08:07:15.831149Z"
    }
   },
   "outputs": [],
   "source": [
    "# 图像路径和文本描述文件的路径\n",
    "image_path = \"./data/deepfashion-multimodal/images/\"\n",
    "train_text = \"./data/deepfashion-multimodal/train_captions.json\"\n",
    "test_text = \"./data/deepfashion-multimodal/test_captions.json\"\n",
    "\n",
    "# 定义一个用于深度学习的数据集类\n",
    "class MyDataset(Dataset):\n",
    "    # 构造函数：初始化数据集\n",
    "    def __init__(self, image_paths, train_captions, test_captions, transform=None):\n",
    "        self.image_paths = image_paths  # 图像文件路径\n",
    "        self.train_captions = train_captions  # 训练集的文本描述\n",
    "        self.test_captions = test_captions  # 测试集的文本描述\n",
    "        self.transform = transform  # 图像的转换方法\n",
    "\n",
    "    # 返回数据集的长度，即图像的数量\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    # 根据索引获取数据集中的一个项\n",
    "    def __getitem__(self, idx):\n",
    "        idx_name = self.image_paths[idx]  # 获取对应索引的图像路径\n",
    "        file_name = idx_name.split(\"/\")[-1]  # 从路径中提取文件名\n",
    "        image = Image.open(idx_name).convert(\"RGB\")  # 打开图像并转换为RGB格式\n",
    "        if self.transform:  # 如果提供了转换方法，则应用转换\n",
    "            image = self.transform(image)\n",
    "        # print(file_name)\n",
    "        return image, self.train_captions[file_name]  # 返回图像及其对应的训练描述\n",
    "    \n",
    "    # 根据索取返回文件名\n",
    "    def get_file_name(self, idx):\n",
    "        idx_name = self.image_paths[idx]  # 获取对应索引的图像路径\n",
    "        file_name = idx_name.split(\"/\")[-1]  # 从路径中提取文件名\n",
    "        return file_name\n",
    "    \n",
    "    # 获取训练集的文本描述\n",
    "    def get_train_captions(self):\n",
    "        return self.train_captions\n",
    "    \n",
    "# 定义一个用于深度学习的数据集类\n",
    "class MyDatasetForTest(Dataset):\n",
    "    # 构造函数：初始化数据集\n",
    "    def __init__(self, image_paths, train_captions, test_captions, transform=None):\n",
    "        self.image_paths = image_paths  # 图像文件路径\n",
    "        self.train_captions = train_captions  # 训练集的文本描述\n",
    "        self.test_captions = test_captions  # 测试集的文本描述\n",
    "        self.transform = transform  # 图像的转换方法\n",
    "\n",
    "    # 返回数据集的长度，即图像的数量\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    # 根据索引获取数据集中的一个项\n",
    "    def __getitem__(self, idx):\n",
    "        idx_name = self.image_paths[idx]  # 获取对应索引的图像路径\n",
    "        file_name = idx_name.split(\"/\")[-1]  # 从路径中提取文件名\n",
    "        image = Image.open(idx_name).convert(\"RGB\")  # 打开图像并转换为RGB格式\n",
    "        if self.transform:  # 如果提供了转换方法，则应用转换\n",
    "            image = self.transform(image)\n",
    "        # print(file_name)\n",
    "        return image, self.test_captions[file_name]  # 返回图像及其对应的训练描述\n",
    "    \n",
    "    # 根据索取返回文件名\n",
    "    def get_file_name(self, idx):\n",
    "        idx_name = self.image_paths[idx]  # 获取对应索引的图像路径\n",
    "        file_name = idx_name.split(\"/\")[-1]  # 从路径中提取文件名\n",
    "        return file_name\n",
    "    \n",
    "    # 获取训练集的文本描述\n",
    "    def get_train_captions(self):\n",
    "        return self.test_captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T08:07:16.884187200Z",
     "start_time": "2023-12-29T08:07:16.844453200Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取训练集和测试集的文本描述\n",
    "train_captions = json.load(open(train_text, 'r'))  # 从JSON文件加载训练集的文本描述\n",
    "test_captions = json.load(open(test_text, 'r'))    # 从JSON文件加载测试集的文本描述\n",
    "\n",
    "image_paths = []  # 初始化用于存储图像路径的列表\n",
    "\n",
    "# 遍历训练集的文本描述，并构建图像的完整路径列表\n",
    "for key in train_captions.keys():\n",
    "    image_paths.append(image_path + key)  # 将图像的基本路径和图像文件名结合，添加到列表中\n",
    "    \n",
    "image_path_test = []\n",
    "# 遍历测试集的文本描述，并构建图像的完整路径列表\n",
    "for key in test_captions.keys():\n",
    "    image_path_test.append(image_path + key)  # 将图像的基本路径和图像文件名结合，添加到列表中\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T08:07:17.618184100Z",
     "start_time": "2023-12-29T08:07:17.613174800Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义一个图像转换流程，包括调整尺寸、转换为张量、标准化\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),  # 首先，调整图像尺寸为256x256像素\n",
    "        transforms.ToTensor(),  # 接着，将图像数据转换为张量（Tensor）格式\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # 最后，对图像进行标准化处理\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T08:07:18.387562Z",
     "start_time": "2023-12-29T08:07:18.147619400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WOMEN-Jackets_Coats-id_00005611-01_4_full.jpg [159, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 13, 14, 15, 16, 17, 1, 18, 3, 16, 14, 5, 19, 1, 8, 16, 20, 9, 15, 4, 10, 11, 12, 21, 22, 23, 24, 25, 26, 27, 28, 7, 8, 9, 29, 12, 21, 30, 16, 31, 32, 33, 34, 35, 36, 21, 30, 4, 37, 158, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "WOMEN-Tees_Tanks-id_00005033-03_4_full.jpg [159, 38, 39, 40, 4, 41, 6, 42, 8, 9, 43, 12, 44, 4, 32, 45, 46, 1, 47, 24, 32, 5, 48, 1, 49, 50, 28, 20, 8, 9, 10, 11, 12, 1, 22, 24, 32, 51, 158, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1\n",
      "WOMEN-Rompers_Jumpsuits-id_00000245-01_1_front.jpg [159, 38, 39, 52, 4, 41, 6, 7, 8, 9, 10, 11, 12, 44, 4, 32, 53, 46, 21, 54, 24, 32, 5, 55, 1, 56, 50, 28, 7, 8, 9, 10, 11, 12, 57, 16, 32, 33, 34, 35, 36, 1, 22, 24, 32, 58, 57, 16, 25, 59, 34, 35, 60, 158, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 2\n",
      "WOMEN-Blouses_Shirts-id_00002333-02_7_additional.jpg [159, 38, 61, 4, 5, 6, 7, 8, 9, 10, 11, 12, 44, 4, 32, 45, 46, 21, 54, 24, 32, 5, 48, 1, 49, 50, 28, 7, 8, 9, 62, 12, 57, 16, 25, 59, 34, 35, 60, 57, 16, 32, 33, 34, 35, 36, 158, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 3\n",
      "WOMEN-Blouses_Shirts-id_00002102-04_4_full.jpg [159, 1, 22, 16, 31, 32, 39, 39, 40, 28, 43, 12, 1, 39, 40, 16, 28, 7, 63, 1, 49, 64, 22, 24, 16, 14, 65, 19, 1, 49, 50, 28, 20, 8, 9, 66, 11, 12, 57, 16, 25, 59, 34, 35, 60, 57, 16, 32, 33, 34, 35, 36, 57, 16, 25, 59, 67, 68, 35, 69, 158, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 4\n"
     ]
    }
   ],
   "source": [
    "# 获取图像描述的键和值\n",
    "image_dic = train_captions.keys()\n",
    "image_descriptions = train_captions.values()\n",
    "\n",
    "# 构建词汇表\n",
    "vocab = Counter()\n",
    "for description in image_descriptions:\n",
    "    vocab.update(description.split())  # 更新词汇表的计数\n",
    "\n",
    "# 设置出现次数阈值，移除低频词汇（这里阈值为-1，意味着不实际移除任何词汇）\n",
    "threshold = -1\n",
    "words = [word for word, count in vocab.items() if count >= threshold]\n",
    "\n",
    "# 为每个单词创建索引映射\n",
    "idx_to_word = {idx: word for idx, word in enumerate(words, 1)}\n",
    "\n",
    "# 添加特殊标记到词汇表\n",
    "idx_to_word[0] = \"<pad>\"  # 填充标记\n",
    "idx_to_word[len(idx_to_word)] = \"<end>\"  # 结束标记\n",
    "idx_to_word[len(idx_to_word)] = \"<start>\"  # 开始标记\n",
    "\n",
    "# 为描述添加开始和结束标记\n",
    "for key, description in train_captions.items():\n",
    "    train_captions[key] = \"<start> \" + description + \" <end>\"\n",
    "\n",
    "# 使用 <pad> 填充文本至最大长度\n",
    "max_length = max(len(description.split()) for description in image_descriptions)\n",
    "for key, description in train_captions.items():\n",
    "    train_captions[key] += \" <pad>\" * (max_length - len(description.split()))\n",
    "for key, description in test_captions.items():\n",
    "    test_captions[key] += \" <pad>\" * (max_length - len(description.split()))\n",
    "\n",
    "# 创建从单词到索引的映射\n",
    "word_to_idx = {word: idx for idx, word in idx_to_word.items()}\n",
    "vocab_size_len = len(idx_to_word)\n",
    "\n",
    "# 将每个单词转换为其索引\n",
    "train_captions = {\n",
    "    key: [word_to_idx[word] for word in value.split()]\n",
    "    for key, value in train_captions.items()\n",
    "}\n",
    "test_captions = {\n",
    "    key: [word_to_idx[word] for word in value.split()]\n",
    "    for key, value in test_captions.items()\n",
    "}\n",
    "\n",
    "# 打印训练集中前五个描述的键值对和索引\n",
    "for key, value, idx in zip(train_captions.keys(), train_captions.values(), range(5)):\n",
    "    print(key, value, idx)\n",
    "    if idx == 4:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T08:07:18.931634700Z",
     "start_time": "2023-12-29T08:07:18.757189200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256]) torch.Size([95])\n",
      "tensor([[[2.2318, 2.2318, 2.2318,  ..., 2.1804, 2.1804, 2.1804],\n",
      "         [2.2318, 2.2147, 2.2318,  ..., 2.1804, 2.1804, 2.1804],\n",
      "         [2.2318, 2.2318, 2.2318,  ..., 2.1804, 2.1804, 2.1804],\n",
      "         ...,\n",
      "         [2.1804, 2.1633, 2.1633,  ..., 2.0777, 2.0777, 2.0777],\n",
      "         [2.1804, 2.1633, 2.1633,  ..., 2.0948, 2.0777, 2.0777],\n",
      "         [2.1804, 2.1804, 2.1633,  ..., 2.0948, 2.0948, 2.0777]],\n",
      "\n",
      "        [[2.4111, 2.4111, 2.4111,  ..., 2.3585, 2.3585, 2.3585],\n",
      "         [2.4111, 2.3936, 2.4111,  ..., 2.3585, 2.3585, 2.3585],\n",
      "         [2.4111, 2.4111, 2.4111,  ..., 2.3585, 2.3585, 2.3585],\n",
      "         ...,\n",
      "         [2.3235, 2.3410, 2.3410,  ..., 2.2185, 2.2185, 2.2185],\n",
      "         [2.3235, 2.3410, 2.3410,  ..., 2.2360, 2.2185, 2.2185],\n",
      "         [2.3235, 2.3410, 2.3410,  ..., 2.2360, 2.2360, 2.2185]],\n",
      "\n",
      "        [[2.6226, 2.6226, 2.6226,  ..., 2.5703, 2.5703, 2.5703],\n",
      "         [2.6226, 2.6051, 2.6226,  ..., 2.5703, 2.5703, 2.5703],\n",
      "         [2.6226, 2.6226, 2.6226,  ..., 2.5703, 2.5703, 2.5703],\n",
      "         ...,\n",
      "         [2.5529, 2.5529, 2.5529,  ..., 2.4483, 2.4483, 2.4483],\n",
      "         [2.5529, 2.5529, 2.5529,  ..., 2.4657, 2.4483, 2.4483],\n",
      "         [2.5529, 2.5529, 2.5529,  ..., 2.4483, 2.4657, 2.4483]]]) tensor([159,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,   1,\n",
      "         13,  14,  15,  16,  17,   1,  18,   3,  16,  14,   5,  19,   1,   8,\n",
      "         16,  20,   9,  15,   4,  10,  11,  12,  21,  22,  23,  24,  25,  26,\n",
      "         27,  28,   7,   8,   9,  29,  12,  21,  30,  16,  31,  32,  33,  34,\n",
      "         35,  36,  21,  30,   4,  37, 158,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])\n"
     ]
    }
   ],
   "source": [
    "# 将训练集的文本描述转换为PyTorch张量\n",
    "for key, value in train_captions.items():\n",
    "    train_captions[key] = torch.tensor(value, dtype=torch.long)\n",
    "\n",
    "# 创建一个数据集实例\n",
    "dataset = MyDataset(image_paths, train_captions, test_captions, transform)\n",
    "dataset_test = MyDatasetForTest(image_path_test, train_captions, test_captions, transform)\n",
    "\n",
    "# 遍历数据集的第一个元素，打印图像和文本描述的形状和内容\n",
    "for i in range(1):\n",
    "    # 打印图像和文本描述的张量形状\n",
    "    print(dataset[i][0].shape, dataset[i][1].shape)\n",
    "    # 打印图像和文本描述的内容\n",
    "    print(dataset[i][0], dataset[i][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T08:07:19.489573300Z",
     "start_time": "2023-12-29T08:07:19.483547200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn import Transformer\n",
    "\n",
    "# 使用预训练的CNN来提取图像特征\n",
    "class FeatureExtractorCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractorCNN, self).__init__()\n",
    "        # 载入预训练的ResNet-18模型\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        # 移除最后一个层，以获取特征\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])\n",
    "\n",
    "    def forward(self, images):\n",
    "        # 调整图像尺寸并重塑\n",
    "        batch_size = images.shape[0]\n",
    "        images = nn.functional.interpolate(\n",
    "            images, scale_factor=3, mode=\"bilinear\", align_corners=False\n",
    "        )\n",
    "        images = images.unfold(2, 256, 256).unfold(3, 256, 256)\n",
    "        images = images.contiguous().view(-1, 3, 256, 256)\n",
    "\n",
    "        # 通过ResNet提取特征\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(batch_size, 9, -1)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T08:07:20.874286900Z",
     "start_time": "2023-12-29T08:07:20.850656600Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# 首先定义注意力机制层\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention = nn.Linear(feature_dim + decoder_dim, attention_dim)\n",
    "        self.v = nn.Linear(attention_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, features, hidden_state):\n",
    "        # 组合来自上一解码状态的特征和隐藏状态\n",
    "        hidden_state = hidden_state.unsqueeze(1).repeat(1, features.size(1), 1)\n",
    "        combined = torch.cat((features, hidden_state), 2)\n",
    "        attention_scores = self.attention(combined)\n",
    "        attention_scores = self.v(torch.tanh(attention_scores)).squeeze(2)\n",
    "        alpha = F.softmax(attention_scores, dim=1)\n",
    "        context = (features * alpha.unsqueeze(2)).sum(dim=1)\n",
    "        return context, alpha\n",
    "\n",
    "# 定义解码器RNN\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.attention = Attention(feature_dim=512, decoder_dim=decoder_dim, attention_dim=attention_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.LSTMCell(embed_dim + 512, decoder_dim, bias=True)\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embedding(captions)\n",
    "        h, c = self.init_hidden_state(features)  # 初始化隐藏状态和细胞状态\n",
    "        seq_length = len(captions[0])\n",
    "        batch_size = features.size(0)\n",
    "        preds = torch.zeros(batch_size, seq_length, self.vocab_size).to(features.device)\n",
    "        \n",
    "        for t in range(seq_length):\n",
    "            context, alpha = self.attention(features, h)\n",
    "            rnn_input = torch.cat((embeddings[:, t, :], context), 1)\n",
    "            h, c = self.rnn(rnn_input, (h, c))\n",
    "            out = self.fc(h)\n",
    "            preds[:, t, :] = out\n",
    "        \n",
    "        return preds\n",
    "\n",
    "    def init_hidden_state(self, features):\n",
    "        mean_features = features.mean(dim=1)\n",
    "        h = torch.zeros(size=(features.size(0), self.decoder_dim)).to(features.device)\n",
    "        c = torch.zeros(size=(features.size(0), self.decoder_dim)).to(features.device)\n",
    "        return h, c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T08:07:21.701775800Z",
     "start_time": "2023-12-29T08:07:21.484743700Z"
    }
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "# check dataset\n",
    "\n",
    "for i, (image, caption) in enumerate(dataloader):\n",
    "    #打印图像和文本描述的张量形状\n",
    "    # print(image.shape, caption.shape)\n",
    "    #打印一个文本描述的内容\n",
    "    # print(caption[0])\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "# torch.Size([16, 3, 256, 256]) torch.Size([16, 95])\n",
    "# tensor([159,   1,   2,   3,   4,  83,   6,   7,   8,   9,  66,  11,  12,  44,\n",
    "#           4,  32,  92,  46,   1,  18,   3,  16,  14,  65,  19,   1,   8,  16,\n",
    "#          20,   9,  15,   4,  10,  11,  12,  57,  16,  32,  33,  34,  35,  36,\n",
    "#         158,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "#           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "#           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "#           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T08:07:22.409942200Z",
     "start_time": "2023-12-29T08:07:22.139194900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda3\\envs\\NNDL_CourseDesign\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "E:\\anaconda3\\envs\\NNDL_CourseDesign\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 实例化模型\n",
    "cnn = FeatureExtractorCNN()\n",
    "cnn = nn.DataParallel(cnn)\n",
    "\n",
    "attention_dim = 256\n",
    "embed_dim = 256\n",
    "decoder_dim = 512\n",
    "vocab_size = vocab_size_len  \n",
    "\n",
    "decoder = DecoderRNN(attention_dim, embed_dim, decoder_dim, vocab_size)\n",
    "decoder =nn.DataParallel(decoder)\n",
    "\n",
    "\n",
    "\n",
    "# 训练参数\n",
    "epoch = 5\n",
    "lr = 0.001\n",
    "# todo:创建优化器\n",
    "optimizer = optim.Adam(list(decoder.parameters()), lr=lr)\n",
    "\n",
    "\n",
    "\n",
    "# check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "decoder.to(device)\n",
    "cnn = cnn.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T08:39:12.442853500Z",
     "start_time": "2023-12-28T08:39:03.672844800Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 43\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;66;03m# todo:反向传播和优化\u001B[39;00m\n\u001B[0;32m     42\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()  \u001B[38;5;66;03m# 计算梯度\u001B[39;00m\n\u001B[1;32m---> 43\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()  \u001B[38;5;66;03m# 更新参数\u001B[39;00m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;66;03m# todo:每10步打印一次损失，呈现一次输出\u001B[39;00m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m10\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32mE:\\anaconda3\\envs\\NNDL_CourseDesign\\Lib\\site-packages\\torch\\optim\\optimizer.py:373\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    368\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    369\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    370\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    371\u001B[0m             )\n\u001B[1;32m--> 373\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    374\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[0;32m    376\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[1;32mE:\\anaconda3\\envs\\NNDL_CourseDesign\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     74\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     75\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[1;32m---> 76\u001B[0m     ret \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     78\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[1;32mE:\\anaconda3\\envs\\NNDL_CourseDesign\\Lib\\site-packages\\torch\\optim\\adam.py:163\u001B[0m, in \u001B[0;36mAdam.step\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    152\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m    154\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[0;32m    155\u001B[0m         group,\n\u001B[0;32m    156\u001B[0m         params_with_grad,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    160\u001B[0m         max_exp_avg_sqs,\n\u001B[0;32m    161\u001B[0m         state_steps)\n\u001B[1;32m--> 163\u001B[0m     adam(\n\u001B[0;32m    164\u001B[0m         params_with_grad,\n\u001B[0;32m    165\u001B[0m         grads,\n\u001B[0;32m    166\u001B[0m         exp_avgs,\n\u001B[0;32m    167\u001B[0m         exp_avg_sqs,\n\u001B[0;32m    168\u001B[0m         max_exp_avg_sqs,\n\u001B[0;32m    169\u001B[0m         state_steps,\n\u001B[0;32m    170\u001B[0m         amsgrad\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mamsgrad\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    171\u001B[0m         beta1\u001B[38;5;241m=\u001B[39mbeta1,\n\u001B[0;32m    172\u001B[0m         beta2\u001B[38;5;241m=\u001B[39mbeta2,\n\u001B[0;32m    173\u001B[0m         lr\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    174\u001B[0m         weight_decay\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mweight_decay\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    175\u001B[0m         eps\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124meps\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    176\u001B[0m         maximize\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmaximize\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    177\u001B[0m         foreach\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mforeach\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    178\u001B[0m         capturable\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcapturable\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    179\u001B[0m         differentiable\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    180\u001B[0m         fused\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfused\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    181\u001B[0m         grad_scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrad_scale\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m    182\u001B[0m         found_inf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound_inf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m    183\u001B[0m     )\n\u001B[0;32m    185\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[1;32mE:\\anaconda3\\envs\\NNDL_CourseDesign\\Lib\\site-packages\\torch\\optim\\adam.py:311\u001B[0m, in \u001B[0;36madam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[0;32m    308\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    309\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[1;32m--> 311\u001B[0m func(params,\n\u001B[0;32m    312\u001B[0m      grads,\n\u001B[0;32m    313\u001B[0m      exp_avgs,\n\u001B[0;32m    314\u001B[0m      exp_avg_sqs,\n\u001B[0;32m    315\u001B[0m      max_exp_avg_sqs,\n\u001B[0;32m    316\u001B[0m      state_steps,\n\u001B[0;32m    317\u001B[0m      amsgrad\u001B[38;5;241m=\u001B[39mamsgrad,\n\u001B[0;32m    318\u001B[0m      beta1\u001B[38;5;241m=\u001B[39mbeta1,\n\u001B[0;32m    319\u001B[0m      beta2\u001B[38;5;241m=\u001B[39mbeta2,\n\u001B[0;32m    320\u001B[0m      lr\u001B[38;5;241m=\u001B[39mlr,\n\u001B[0;32m    321\u001B[0m      weight_decay\u001B[38;5;241m=\u001B[39mweight_decay,\n\u001B[0;32m    322\u001B[0m      eps\u001B[38;5;241m=\u001B[39meps,\n\u001B[0;32m    323\u001B[0m      maximize\u001B[38;5;241m=\u001B[39mmaximize,\n\u001B[0;32m    324\u001B[0m      capturable\u001B[38;5;241m=\u001B[39mcapturable,\n\u001B[0;32m    325\u001B[0m      differentiable\u001B[38;5;241m=\u001B[39mdifferentiable,\n\u001B[0;32m    326\u001B[0m      grad_scale\u001B[38;5;241m=\u001B[39mgrad_scale,\n\u001B[0;32m    327\u001B[0m      found_inf\u001B[38;5;241m=\u001B[39mfound_inf)\n",
      "File \u001B[1;32mE:\\anaconda3\\envs\\NNDL_CourseDesign\\Lib\\site-packages\\torch\\optim\\adam.py:506\u001B[0m, in \u001B[0;36m_multi_tensor_adam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[0;32m    503\u001B[0m         device_grads \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39m_foreach_add(device_grads, device_params, alpha\u001B[38;5;241m=\u001B[39mweight_decay)\n\u001B[0;32m    505\u001B[0m \u001B[38;5;66;03m# Decay the first and second moment running average coefficient\u001B[39;00m\n\u001B[1;32m--> 506\u001B[0m torch\u001B[38;5;241m.\u001B[39m_foreach_lerp_(device_exp_avgs, device_grads, \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta1)\n\u001B[0;32m    508\u001B[0m torch\u001B[38;5;241m.\u001B[39m_foreach_mul_(device_exp_avg_sqs, beta2)\n\u001B[0;32m    509\u001B[0m torch\u001B[38;5;241m.\u001B[39m_foreach_addcmul_(device_exp_avg_sqs, device_grads, device_grads, \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta2)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# 损失函数，忽略填充索引\n",
    "pad_index = 0\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)\n",
    "\n",
    "#训练循环\n",
    "for e in range(epoch):\n",
    "    for i, (image, caption) in enumerate(dataloader):\n",
    "        # 将图像和描述移至计算设备\n",
    "        image = image.to(device)\n",
    "        caption = caption.to(device)\n",
    "\n",
    "        # 重置梯度\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 提取图像特征\n",
    "        features = cnn(image)\n",
    "        # src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(features, caption[:, :-1])\n",
    "        \n",
    "        # 打印图像特征的形状\n",
    "        # print(features.shape)\n",
    "        # 模型前向传播\n",
    "        outputs = decoder(features, caption[:, :-1])\n",
    "        \n",
    "        # #打印输出的形状\n",
    "        # print(\"outputs\",outputs.shape)\n",
    "        # print(outputs)\n",
    "        # #打印caption的形状\n",
    "        # print(\"caption:\",caption.shape)\n",
    "        # print(caption)\n",
    "        \n",
    "        loss = criterion(outputs.reshape(-1, 160), caption[:, 1:].reshape(-1))\n",
    "\n",
    "        # 反向传播和优化\n",
    "        loss.backward()  # 计算梯度\n",
    "        optimizer.step()  # 更新参数\n",
    "\n",
    "        # 每10步打印一次损失，呈现一次输出\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch {e}, Step {i}, Loss: {loss.item()}\")\n",
    "            \n",
    "            outputs_str = []\n",
    "            outputs_str.append([])\n",
    "            for j in range(outputs.shape[1]):\n",
    "                outputs_str[0].append(idx_to_word[torch.argmax(outputs[0][j]).item()])\n",
    "            print(\"outputs_str:\",outputs_str)\n",
    "            \n",
    "            captions_str = []\n",
    "            captions_str.append([])\n",
    "            for j in range(caption.shape[1]):\n",
    "                captions_str[0].append(idx_to_word[caption[0][j].item()])\n",
    "            print(\"captions_str:\",captions_str)\n",
    "\n",
    "    # 保存模型至当前目录的model_a文件夹下\n",
    "    # torch.save({\n",
    "    #     'epoch': e,\n",
    "    #     'decoder_state_dict': decoder.state_dict(),\n",
    "    #     \n",
    "    #     'optimizer_state_dict': optimizer.state_dict(),\n",
    "    # }, f'model_a/checkpoint_{e}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Running on local URL:  http://127.0.0.1:7876\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<div><iframe src=\"http://127.0.0.1:7876/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": ""
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 写一个简单的前端\n",
    "import gradio as gr\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "def generate_caption(model, image_features, word2idx, idx2word, max_length_inf=93):\n",
    "    outputs_inf = [word2idx[\"<start>\"]]\n",
    "    image_features.to(device)\n",
    "\n",
    "    for k in range(max_length_inf - 1):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # 将当前生成的字幕转换为张量\n",
    "            current_caption = torch.LongTensor(outputs_inf).unsqueeze(0).to(device)\n",
    "            \n",
    "            # 生成下一个单词的预测\n",
    "            out = model(image_features, current_caption)\n",
    "\n",
    "        # 获取最后一个时间步的最大值索引\n",
    "        last_time_step_max_index = out.argmax(dim=2)[0, -1].item()\n",
    "\n",
    "        next_word = last_time_step_max_index\n",
    "\n",
    "\n",
    "        # 检查是否达到结束标记\n",
    "        if next_word == word2idx['<end>']:\n",
    "            break  # 一旦生成<end>标记，立即停止生成\n",
    "\n",
    "        outputs_inf.append(next_word)\n",
    "        \n",
    "    # 转换序列为文字（跳过第一个元素）\n",
    "    caption_inf = [idx2word[idx] for idx in outputs_inf[1:]]\n",
    "\n",
    "    return ' '.join(caption_inf)\n",
    "\n",
    "\n",
    "def predict(images):\n",
    "    #对图像进行预处理\n",
    "    images = Image.fromarray(images)\n",
    "    images = transform(images)\n",
    "    images = images.unsqueeze(0)\n",
    "    # 使用模型对图像进行描述生成    \n",
    "    image_feature = cnn(images)\n",
    "    # print(test_feature.shape)\n",
    "    image_feature.to(device)\n",
    "\n",
    "    output = generate_caption(decoder_inf, image_feature, word_to_idx, idx_to_word)\n",
    "    # 返回描述文本\n",
    "    description = output\n",
    "    \n",
    "    return description\n",
    "\n",
    "\n",
    "# load model\n",
    "\n",
    "decoder_inf = DecoderRNN(attention_dim=256, embed_dim=256, decoder_dim=512, vocab_size=160)\n",
    "decoder_inf.to(device)\n",
    "checkpoint = torch.load(f'model_a/checkpoint_3.pth')\n",
    "decoder_inf.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "decoder_inf.eval()\n",
    "\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=\"image\",\n",
    "    outputs=\"text\",\n",
    "    live=True,  # 实时更新输出\n",
    "    title=\"图片描述模型\",\n",
    "    description=\"上传一张图片，获取描述文本。\",\n",
    ")\n",
    "\n",
    "interface.launch()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T08:34:13.578326300Z",
     "start_time": "2023-12-29T08:34:13.155644400Z"
    }
   },
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([3, 256, 256]) torch.Size([95])\n",
      "WOMEN-Tees_Tanks-id_00002951-02_7_additional.jpg\n",
      "tensor([159,  21,  47,  24,  32, 126,  40,  28,  11, 133,  79,   9,  32,   5,\n",
      "         48,   1,  40,  16,  28,   7,   8,   9,  72,  13,  16,  84,   1,  49,\n",
      "         50,  28,  20,   8,   9,  10,  11,  12, 158,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])\n",
      "torch.Size([1, 9, 512])\n",
      "<start> The female is wearing a short-sleeve shirt with solid color patterns. The shirt is with cotton fabric. It has a round neckline. The pants the female wears is of long length. The pants are with denim fabric and pure color patterns. The female is wearing a ring on her finger.\n",
      "train_captions_str: [['<start>', 'This', 'person', 'wears', 'a', 'medium-sleeve', 'shirt', 'with', 'color', 'block', 'patterns', 'and', 'a', 'long', 'pants.', 'The', 'shirt', 'is', 'with', 'cotton', 'fabric', 'and', 'its', 'neckline', 'is', 'round.', 'The', 'pants', 'are', 'with', 'denim', 'fabric', 'and', 'solid', 'color', 'patterns.', '<end>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]\n"
     ]
    }
   ],
   "source": [
    "# 生成训练集中的描述\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "def generate_caption(model, image_features, word2idx, idx2word, max_length_inf=93):\n",
    "    outputs_inf = [word2idx[\"<start>\"]]\n",
    "    image_features.to(device)\n",
    "    # print_tensor_devices(image_features)\n",
    "\n",
    "    for k in range(max_length_inf - 1):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            \n",
    "            # 将当前生成的字幕转换为张量\n",
    "            current_caption = torch.LongTensor(outputs_inf).unsqueeze(0).to(device)\n",
    "            \n",
    "            # 生成下一个单词的预测\n",
    "            out = model(image_features, current_caption)\n",
    "            #打印out的形状\n",
    "            # print(\"out:\",out.shape)\n",
    "        # 获取最后一个时间步的最大值索引\n",
    "        last_time_step_max_index = out.argmax(dim=2)[0, -1].item()\n",
    "\n",
    "        next_word = last_time_step_max_index\n",
    "\n",
    "\n",
    "        # 检查是否达到结束标记\n",
    "        if next_word == word2idx['<end>']:\n",
    "            break  # 一旦生成<end>标记，立即停止生成\n",
    "\n",
    "        outputs_inf.append(next_word)\n",
    "        \n",
    "\n",
    "    # 转换序列为文字\n",
    "    caption_inf = [idx2word[idx] for idx in outputs_inf]\n",
    "\n",
    "    return ' '.join(caption_inf)\n",
    "\n",
    "# load model\n",
    "\n",
    "decoder_inf = DecoderRNN(attention_dim=256, embed_dim=256, decoder_dim=512, vocab_size=160)\n",
    "decoder_inf.to(device)\n",
    "checkpoint = torch.load(f'model_a/checkpoint_3.pth')\n",
    "decoder_inf.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "decoder_inf.eval()\n",
    "\n",
    "# test\n",
    "test_data = dataset[10154]\n",
    "train_captions = test_data[1]\n",
    "# print(test_data[0].shape, test_data[1].shape)# 3,256,256 95\n",
    "\n",
    "file_name = dataset.get_file_name(10154)\n",
    "print(file_name)\n",
    "\n",
    "# print(train_captions)\n",
    "\n",
    "test_image = test_data[0].unsqueeze(0).to(device)\n",
    "test_feature = cnn(test_image)\n",
    "# print(test_feature.shape)\n",
    "\n",
    "test_feature.to(device)\n",
    "\n",
    "output = generate_caption(decoder_inf, test_feature, word_to_idx, idx_to_word)\n",
    "\n",
    "print(output)\n",
    "\n",
    "train_captions_str = []\n",
    "train_captions_str.append([])\n",
    "for j in range(train_captions.shape[0]):\n",
    "    train_captions_str[0].append(idx_to_word[train_captions[j].item()])\n",
    "print(\"train_captions_str:\",train_captions_str)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:15:06.409830800Z",
     "start_time": "2023-12-29T02:15:04.670278800Z"
    }
   },
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([3, 256, 256])\n",
      "WOMEN-Tees_Tanks-id_00002201-18_7_additional.jpg\n",
      "tensor([  1,   2,   3,   4,  83,   6,   7,   8,   9,  43,  12,  44,   4,  32,\n",
      "         92,  46,   1,  18,   3,  16,  14,  83,  19,   1,   8,  16, 100,   9,\n",
      "         15,   4,  10,  11,  12,  21, 122,   4,  32,  93,  67,  68,  94,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])\n",
      "torch.Size([1, 9, 512])\n",
      "The upper clothing has short sleeves, cotton fabric and graphic patterns. It has a round neckline. The lower clothing is of short length. The fabric is cotton and it has pure color patterns. The person wears a ring. There is an accessory on her wrist.\n",
      "test_captions_str: [['The', 'upper', 'clothing', 'has', 'short', 'sleeves,', 'cotton', 'fabric', 'and', 'graphic', 'patterns.', 'It', 'has', 'a', 'crew', 'neckline.', 'The', 'lower', 'clothing', 'is', 'of', 'short', 'length.', 'The', 'fabric', 'is', 'leather', 'and', 'it', 'has', 'solid', 'color', 'patterns.', 'This', 'guy', 'has', 'a', 'hat', 'in', 'his', 'head.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]\n"
     ]
    }
   ],
   "source": [
    "# 生成测试集中的描述\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "def generate_caption(model, image_features, word2idx, idx2word, max_length_inf=93):\n",
    "    outputs_inf = [word2idx[\"<start>\"]]\n",
    "    image_features.to(device)\n",
    "\n",
    "    for k in range(max_length_inf - 1):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # 将当前生成的字幕转换为张量\n",
    "            current_caption = torch.LongTensor(outputs_inf).unsqueeze(0).to(device)\n",
    "            \n",
    "            # 生成下一个单词的预测\n",
    "            out = model(image_features, current_caption)\n",
    "\n",
    "        # 获取最后一个时间步的最大值索引\n",
    "        last_time_step_max_index = out.argmax(dim=2)[0, -1].item()\n",
    "\n",
    "        next_word = last_time_step_max_index\n",
    "\n",
    "\n",
    "        # 检查是否达到结束标记\n",
    "        if next_word == word2idx['<end>']:\n",
    "            break  # 一旦生成<end>标记，立即停止生成\n",
    "\n",
    "        outputs_inf.append(next_word)\n",
    "        \n",
    "    # 转换序列为文字（跳过第一个元素）\n",
    "    caption_inf = [idx2word[idx] for idx in outputs_inf[1:]]\n",
    "\n",
    "    return ' '.join(caption_inf)\n",
    "\n",
    "# load model\n",
    "\n",
    "decoder_inf = DecoderRNN(attention_dim=256, embed_dim=256, decoder_dim=512, vocab_size=160)\n",
    "decoder_inf.to(device)\n",
    "checkpoint = torch.load(f'model_a/checkpoint_3.pth')\n",
    "decoder_inf.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "decoder_inf.eval()\n",
    "\n",
    "# test\n",
    "test_data_test = dataset_test[10]\n",
    "test_captions = test_data_test[1]\n",
    "test_captions = torch.tensor(test_captions)\n",
    "\n",
    "# print(test_data_test[0].shape)\n",
    "file_name = dataset_test.get_file_name(10)\n",
    "print(file_name)\n",
    "# print(test_captions)\n",
    "\n",
    "test_image = test_data_test[0].unsqueeze(0).to(device)\n",
    "test_feature = cnn(test_image)\n",
    "# print(test_feature.shape)\n",
    "\n",
    "test_feature.to(device)\n",
    "\n",
    "\n",
    "output = generate_caption(decoder_inf, test_feature, word_to_idx, idx_to_word)\n",
    "\n",
    "print(output)\n",
    "\n",
    "test_captions_str = []\n",
    "test_captions_str.append([])\n",
    "for j in range(test_captions.shape[0]):\n",
    "    test_captions_str[0].append(idx_to_word[test_captions[j].item()])\n",
    "print(\"test_captions_str:\",test_captions_str)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:44:49.635143300Z",
     "start_time": "2023-12-29T02:44:48.676900400Z"
    }
   },
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjson\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(device)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_caption\u001B[39m(model, image_features, word2idx, idx2word, max_length_inf\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m93\u001B[39m):\n",
      "\u001B[1;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# 跑完测试集，生成json文件\n",
    "import json\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "def generate_caption(model, image_features, word2idx, idx2word, max_length_inf=93):\n",
    "    outputs_inf = [word2idx[\"<start>\"]]\n",
    "    image_features.to(device)\n",
    "\n",
    "    for k in range(max_length_inf - 1):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # 将当前生成的字幕转换为张量\n",
    "            current_caption = torch.LongTensor(outputs_inf).unsqueeze(0).to(device)\n",
    "            \n",
    "            # 生成下一个单词的预测\n",
    "            out = model(image_features, current_caption)\n",
    "\n",
    "        # 获取最后一个时间步的最大值索引\n",
    "        last_time_step_max_index = out.argmax(dim=2)[0, -1].item()\n",
    "\n",
    "        next_word = last_time_step_max_index\n",
    "\n",
    "\n",
    "        # 检查是否达到结束标记\n",
    "        if next_word == word2idx['<end>']:\n",
    "            break  # 一旦生成<end>标记，立即停止生成\n",
    "\n",
    "        outputs_inf.append(next_word)\n",
    "        \n",
    "    # 转换序列为文字（跳过第一个元素）\n",
    "    caption_inf = [idx2word[idx] for idx in outputs_inf[1:]]\n",
    "\n",
    "    return ' '.join(caption_inf)\n",
    "\n",
    "# load model\n",
    "\n",
    "decoder_inf = DecoderRNN(attention_dim=256, embed_dim=256, decoder_dim=512, vocab_size=160)\n",
    "decoder_inf.to(device)\n",
    "checkpoint = torch.load(f'model_a/checkpoint_3.pth')\n",
    "decoder_inf.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "decoder_inf.eval()\n",
    "\n",
    "# test\n",
    "# 创建字典\n",
    "data = {}\n",
    "\n",
    "for i in range(2538):\n",
    "    test_data_test = dataset_test[i]\n",
    "    test_captions = test_data_test[1]\n",
    "    test_captions = torch.tensor(test_captions)\n",
    "\n",
    "    file_name = dataset_test.get_file_name(i)\n",
    "\n",
    "    test_image = test_data_test[0].unsqueeze(0).to(device)\n",
    "    test_feature = cnn(test_image)\n",
    "\n",
    "    test_feature.to(device)\n",
    "\n",
    "    output = generate_caption(decoder_inf, test_feature, word_to_idx, idx_to_word)\n",
    "    # 将生成的字幕添加到字典中\n",
    "    data[file_name] = output\n",
    "    # 每隔10个打印一次\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Step {i}, Caption: {output}\")\n",
    "    \n",
    "    \n",
    "# 将字典转换为 JSON 字符串\n",
    "json_data = json.dumps(data, indent=2)\n",
    "# print(json_data)\n",
    "#将json_data写入文件\n",
    "with open('results.json', 'w') as f:\n",
    "    f.write(json_data)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T07:28:07.146764200Z",
     "start_time": "2023-12-29T07:28:06.647412400Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs_str: [['<pad>', 'The', 'upper', 'clothing', 'has', 'long', 'sleeves,', 'cotton', 'fabric', 'and', 'solid', 'color', 'patterns.', 'neckline', 'of', 'it', 'is', 'v-shape.', 'lower', 'length.', 'denim', 'This', 'lady', 'also', 'wears', 'an', 'outer', 'clothing,', 'with', 'complicated', 'female', 'wearing', 'a', 'ring', 'on', 'her', 'finger.', 'neckwear.', 'Her', 'tank', 'shirt', 'no', 'chiffon', 'graphic', 'It', 'round', 'neckline.', 'person', 'pants.', 'pants', 'are', 'ring.', 'top', 'v-shape', 'woman', 'trousers.', 'trousers', 'There', 'belt.', 'accessory', 'wrist.', 'sweater', 'lattice', 'fabric.', 'the', 'three-point', 'pure', 'in', 'his', 'neck.', 'long-sleeve', 'plaid', 'its', 'lapel.', 'socks', 'shoes.', 'suspenders', 'short-sleeve', 'T-shirt', 'patterns', 'shorts.', 'crew.', 'shorts', 'short', 'round.', 'sleeveless', 'floral', 'hat.', 'this', 'pair', 'socks.', 'three-quarter', 'crew', 'hat', 'head.', 'lapel', 'sleeves', 'trousers,', 'pants,', 'waist.', 'leather', 'cotton.', 'pattern', 'graphic.', 'shorts,', 'cut', 'off,', 'off', 'suspenders.', 'medium', 'knitting', 'gentleman', 'other', 'mixed', 'stripe', 'cotton,', 'skirt.', 'skirt', 'striped', 'chiffon.', 'color.', 'sunglasses.', 'guy', 'stand', 'man', 'floral.', 'medium-sleeve', 'square', 'skirt,', 'belt', 'leggings.', 'chiffon,', 'furry', 'block', 'His', 'stripe.', 'stand.', 'square.', 'knitting.', 'complicated.', 'striped.', 'block.', 'plaid.', 'glasses', 'hands', 'or', 'clothes.', 'other,', 'knitting,', 'lattice.', 'denim,', 'other.', 'furry.', 'furry,', 'length', 'denim.', 'leather.', 'mixed,', '<end>', '<start>']]\n",
      "词典里一共有 157 个词。\n"
     ]
    }
   ],
   "source": [
    "# 一个用来看词典的块\n",
    "outputs_str = []\n",
    "outputs_str.append([])\n",
    "for j in range(160):\n",
    "    outputs_str[0].append(idx_to_word[j])\n",
    "print(\"outputs_str:\",outputs_str)\n",
    "# 提取所有的词  \n",
    "all_words = [word for sublist in outputs_str for word in sublist if word != '<pad>' and word != '<start>' and word != '<end>']  \n",
    "  \n",
    "# 计算词的数量  \n",
    "word_count = len(all_words)  \n",
    "  \n",
    "print(f\"词典里一共有 {word_count} 个词。\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T02:53:00.205347300Z",
     "start_time": "2023-12-28T02:53:00.196716500Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 测试指标\n",
    "import evaluate\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "test_text = \"./data/deepfashion-multimodal/test_captions.json\"\n",
    "\n",
    "# read from result.json\n",
    "test_data = json.load(open(\"./results.json\", 'r'))\n",
    "real_data = json.load(open(test_text, 'r'))\n",
    "\n",
    "test_selected_data = {}\n",
    "real_selected_data = {}\n",
    "\n",
    "indice = 0\n",
    "for key, value in test_data.items():\n",
    "    real_selected_data[key] = real_data[key]\n",
    "    test_selected_data[key] = value\n",
    "    indice += 1\n",
    "\n",
    "eval = evaluate.DeepFashionEvalCap(real_selected_data, test_selected_data)\n",
    "eval.evaluate()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
